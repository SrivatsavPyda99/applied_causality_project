{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas\n",
    "from datetime import datetime\n",
    "import json\n",
    "import twitter\n",
    "import tweepy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = dict()\n",
    "\n",
    "# Enter your own consumer_key, consumer_secret, access_key and access_secret\n",
    "\n",
    "info['CONSUMER_KEY'] = 'kzH6JHce26l6HhuMzHJWW8gDp'\n",
    "info['CONSUMER_SECRET'] = 'QLwzCe6lNG1JriDGvn7gxGgrngtyWrPsXliuPNIlx0TGT4X7Rb'\n",
    "info['ACCESS_KEY'] = '1094854919377637376-BL2OBuEtJCtQn6bzvEWcFiEzKn1pIf'\n",
    "info['ACCESS_SECRET'] = 'oznzrzp4yNto96xFbyhZs4WYlJIYr5mn2fGr42gISasbi'\n",
    "\n",
    "api = twitter.Api(consumer_key=info['CONSUMER_KEY'],\n",
    "                      consumer_secret=info['CONSUMER_SECRET'],\n",
    "                      access_token_key=info['ACCESS_KEY'],\n",
    "                      access_token_secret=info['ACCESS_SECRET'])\n",
    "\n",
    "auth = tweepy.OAuthHandler(info['CONSUMER_KEY'], info['CONSUMER_SECRET'])\n",
    "auth.set_access_token(info['ACCESS_KEY'], info['ACCESS_SECRET'])\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "# Dictionary mapping day distance from June 04, 2010 to business day difference from June 04, 2010\n",
    "\n",
    "start_date = datetime.strptime(\"2010-07-01-+0000\", \"%Y-%m-%d-%z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: empty data matrix, csv reader object containing stock price information\n",
    "# Output: data matrix containing:\n",
    "# list of datetime objects marking 5 pm on the given day\n",
    "# 1) number of days leading into this day from previous business day\n",
    "# 2) closing stock price from previous day\n",
    "# 3) change in stock price leading into previous day\n",
    "# 5) change in stock price leading into current day\n",
    "\n",
    "def stocks_data_fill(data):\n",
    "    final_data = np.zeros((data.shape[0], 6))\n",
    "    day_list = []\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    while j < data.shape[0]:\n",
    "        current_date = datetime.strptime(data[j, 0]+\"-17:00:00-+0000\", \"%Y-%m-%d-%X-%z\")\n",
    "        #current_date = (current_date - start_date).days\n",
    "        \n",
    "        if (current_date - start_date).days >= 0:\n",
    "            day_list.append(current_date)\n",
    "            previous_date = datetime.strptime(data[j-1, 0]+\"-17:00:00-+0000\", \"%Y-%m-%d-%X-%z\")\n",
    "            final_data[i,0] = (current_date - previous_date).days\n",
    "            final_data[i,1] = data[j-1, 4]\n",
    "            final_data[i, 2] = data[j-1, 4] - data[j-2, 4]\n",
    "            final_data[i, 4] = data[j, 4] - data[j-1, 4]\n",
    "            \n",
    "            #final_data[i, 0] = int(data[j-1, 0].replace(\"-\",\"\"))\n",
    "            #final_data[i, 1] = current_date\n",
    "            #final_data[i, 2] = data[j-1, 4]\n",
    "            #final_data[i, 3] = data[j-1, 4] - data[j-2, 4]\n",
    "            #final_data[i, 6] = data[j, 4] - data[j-1, 4]\n",
    "            i = i + 1\n",
    "        j = j + 1\n",
    "    return final_data, day_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output: list of tweet timestamps and text\n",
    "# Method: Manage rate limit by doing 100 tweets/call and implementing error handling. Auto-wait 15 mins before \n",
    "# making additional calls if rate limit has been reached\n",
    "\n",
    "def make_tweet_matrix():\n",
    "    with open('all_ids.json') as json_file:  \n",
    "        tweet_ids = json.load(json_file)\n",
    "        \n",
    "    tweet_timestamps = []\n",
    "    tweet_text = []\n",
    "    \n",
    "    i = 0\n",
    "    iter_amt = 100\n",
    "    \n",
    "    while i < len(tweet_ids):\n",
    "        try:\n",
    "            if (i + iter_amt) < len(tweet_ids):\n",
    "                tweets = tweet_ids[i:i + iter_amt]\n",
    "            else:\n",
    "                tweets = tweet_ids[i:len(tweet_ids)]\n",
    "            tweets = api.statuses_lookup(tweets)\n",
    "        except tweepy.error.TweepError:\n",
    "            print(\"out of quota, waiting 15 minutes\")\n",
    "            time.sleep(60 * 15)\n",
    "            continue\n",
    "            \n",
    "        for tweet in tweets:\n",
    "            json_tweet = tweet._json\n",
    "            tweet_time = json_tweet['created_at']\n",
    "            tweet_date = datetime.strptime(tweet_time, \"%a %b %d %X %z %Y\")\n",
    "            \n",
    "            distance = (tweet_date - start_date).days\n",
    "            if distance < -1:\n",
    "                continue\n",
    "            #tweets[0] = (tweet_date - start_date).days\n",
    "            #tweets[1] = \n",
    "            #tweets[1] = tweet\n",
    "            tweet_timestamps.append(tweet_date)\n",
    "            tweet_text.append(json_tweet['text'])\n",
    "\n",
    "        i = i + iter_amt\n",
    "        \n",
    "    return tweet_timestamps, tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: data matrix filled in with first 3 features and the output, list containing timestamp objects for each row in data matrix,\n",
    "# lists containing tweet timestamps and contents\n",
    "# Output: data matrix containing:\n",
    "# 1) number of days leading into this day from previous business day\n",
    "# 2) closing stock price from previous day\n",
    "# 3) change in stock price leading into previous day\n",
    "# 4) Number of tweets between previous day closing time and current day closing time\n",
    "# 5) change in stock price leading into current day\n",
    "# list of datetime objects marking 5 pm on the given day\n",
    "# list containing concatonated contents of tweets between previous day closing time and current day closing time\n",
    "# Method: Iterate through twitter data and data matrix simultaneously, constructing matrix as we go\n",
    "\n",
    "def tweets_data_fill(data, day_list, tweet_times, tweet_contents):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    tweet_text = []\n",
    "    \n",
    "    while i < len(data):\n",
    "        tweets = []\n",
    "        while tweet_times[j] < day_list[i]:\n",
    "            tweets.append(tweet_contents[j])\n",
    "            data[i, 3] = data[i, 3] + 1\n",
    "            j = j + 1\n",
    "        tweet_text.append(tweets)\n",
    "        i = i + 1\n",
    "    \n",
    "    return data, day_list, tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use helper methods above to return the following information\n",
    "# Output: data matrix containing:\n",
    "# 1) number of days leading into this day from previous business day\n",
    "# 2) closing stock price from previous day\n",
    "# 3) change in stock price leading into previous day\n",
    "# 4) Number of tweets between previous day closing time and current day closing time\n",
    "# 5) change in stock price leading into current day\n",
    "# list of datetime objects marking 5 pm on the given day\n",
    "# list containing concatonated contents of tweets between previous day closing time and current day closing time\n",
    "\n",
    "def prep_data():\n",
    "    data_unformatted = pandas.read_csv('tesla_stock_prices.csv').as_matrix()\n",
    "    data, day_list = stocks_data_fill(data_unformatted)\n",
    "    data = data[~(data==0).all(1)]\n",
    "    \n",
    "    \n",
    "    tweet_times, tweet_contents = make_tweet_matrix()\n",
    "    my_order = [x for y, x in sorted(zip(tweet_times, range(len(tweet_times))))]\n",
    "    tweet_times = [tweet_times[i] for i in my_order]\n",
    "    tweet_contents = [tweet_contents[i] for i in my_order]\n",
    "    \n",
    "    return tweets_data_fill(data, day_list, tweet_times, tweet_contents)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data matrix to a npy file\n",
    "# Write timestamps for each row to a file\n",
    "# Write tweets to a file\n",
    "\n",
    "def write_data():\n",
    "    data, day_list, tweet_contents = prep_data()\n",
    "    day_list = [x.strftime(\"%Y-%m-%d-%X-%z\") for x in day_list]\n",
    "    \n",
    "    np_path = \"data/data.npy\"\n",
    "    day_path = \"data/dates.json\"\n",
    "    tweet_contents_path = \"data/tweets.json\"\n",
    "    \n",
    "    \n",
    "    np.save(np_path, data)\n",
    "    \n",
    "    json_day_contents = json.dumps(day_list)\n",
    "    with open(day_path, 'w') as outfile:\n",
    "        json.dump(json_day_contents, outfile)\n",
    "    \n",
    "    json_tweet_contents = json.dumps(tweet_contents)\n",
    "    with open(tweet_contents_path, 'w') as outfile:\n",
    "        json.dump(json_tweet_contents, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  7., 24.,  2.])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type 'ndarray' is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-e8d1924e6d4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/srivatsavpyda/anaconda/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/srivatsavpyda/anaconda/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/srivatsavpyda/anaconda/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m/Users/srivatsavpyda/anaconda/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n\u001b[0;32m--> 180\u001b[0;31m                         o.__class__.__name__)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type 'ndarray' is not JSON serializable"
     ]
    }
   ],
   "source": [
    "json_data = json.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
